# 检索增强生成（RAG）
RAG（Retrieval-Augmented Generation，检索增强生成）由Facebook在2020年发表的论文《Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》中提出，应用于知识敏感的NLP任务，如问答。   
RAG将问题求解划分为检索和生成两阶段，先通过检索，查找与问题相关的文档，再将文档和问题一并输入模型，由模型推理给出最终的答案，从而解决模型无法扩展知识和产生“幻觉”的问题。目前，RAG架构已逐步应用于各个领域，比如在金融领域，对于某公司的财务分析问题，可以从该公司的财报中查找和问题相关的段落，将其和问题一并输入模型，由模型推理给出财务分析结果。目前，很多开源的大模型解决方案也支持RAG架构，例如LangChain中的RetrievalQA。

本文是对论文《Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》的阅读笔记，并基于LangChain中的RetrievalQA给出一个简单的示例。

## 介绍
预训练语言模型可以从数据中学习知识，在不访问外部知识库的情况下，直接作为参数化的隐式知识库，但其也存在以下缺点：模型无法扩展或修改知识，比如用某天前的数据预训练的模型无法直接回答该天后发生的事实，并且模型可能产生“幻觉”，比如给出和事实不一致的问答。   
因此，论文提出了RAG（Retrieval-Augmented Generation，检索增强生成）架构，如图1所示，其包括参数记忆（预训练语言模型作为生成器）与非参数记忆（预训练文档检索器）两部分。   
对于问答，例如回答“Define 'middle ear'”这一问题，RAG架构首先将问题输入非参数记忆部分。非参数记忆部分包含两个子部分：查询编码器（Query Encoder） ，其将问题进行向量化，文档索引（Document Index），其预先通过另一个编码器将文档进行向量化，并构建文档向量索引。问题输入非参数记忆部分后，先通过查询编码器转化为问题向量，然后从文档向量索引中以最大内积搜索（Maximum Inner Product Search，MIPS）方式查找前K个文档。RAG再将查找出的K个文档和问题合并输入非参数记忆部分。非参数记忆部分即预训练语言模型，其推理生成相应的问答，例如“The middle ear includes the tympanic cavity and the three ossicles.”

RAG 由两部分组成，第一部分负责根据 query x检索出 top-k 个匹配的文档zi，第二部分将 query 和文档拼接起来送入 seq2seq 模型，生成回复y。

### 第一部分：Retriever
在第一部分 Retriever 中，RAG 通过两个不同的 BERT 把外部知识和 query 嵌入为稠密向量，做内积得到内积最大的 k 个文档。将 query 和文档拼接起来组成 k 个输入，作为第二部分的输入。

————————————————
版权声明：本文为CSDN博主「iLuz」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_43214867/article/details/122384404
方法
使用公式描述RAG架构。令问题的词元序列为 
 ，文档检索器为 
 ，其根据 
 返回最相关的K个文档 
 ，生成器 
 ，其采用自回归的方式，根据 
 和 
 ，以及回答的词元序列的前 
 个词元，预测第 
 个词元，直至生成完整的回答。

模型
论文采用端到端的方式对检索器和生成器进行联合训练，并且采用两种模型。 第一种模型是RAG-Sequence模型，可用以下公式表示：
第二种模型是RAG-Token模型，可用以下公式表示：
从公式对这两种模型进行理解，RAG-Sequence模型先计算每个文档条件下回答词元的概率分布，再连乘得到每个文档条件下回答的概率分布，最后再求和得到所有最相关文档条件下回答的概率分布，而RAG-Token模型先计算每个文档条件下回答词元的概率分布，再求和得到所有最相关文档条件下回答词元的概率分布，最后再连乘得到所有最相关文档条件下回答的概率分布。



我们研究了RAG模型，该模型使用输入序列x来检索文本文档z，并在生成目标序列y时将其用作附加上下文。如图1所示，我们的模型利用了两个组件：（i）具有参数η \etaη的检索器p η ( z ∣ x ) p_{\eta}(z|x)p 
η
​
 (z∣x)，它在给定查询x的文本段落上返回（top-K截断）分布；（ii）生成器p θ ( y i ∣ x , z , y 1 : i − 1 ) p_{\theta}(y_i|x,z,y_{1:i-1})p 
θ
​
 (y 
i
​
 ∣x,z,y 
1:i−1
​
 ) 参数化依据θ \thetaθ，其基于先前i-1令牌, y 1 : i − 1 ,y_{1:i-1},y 
1:i−1
​
 的上下文、原始输入x和检索到的段落z来生成当前令牌。

为了端到端地训练检索器和生成器，我们将检索到的文档视为潜在变量
我们提出了两个模型，以不同的方式对潜在文档进行边缘化，以在生成的文本上产生分布。在一种方法RAG-Sequence中，模型使用相同的文档来预测每个目标令牌。第二种方法，RAG-Token，可以基于不同的文档预测每个目标token。在下文中，我们正式介绍了这两个模型，然后描述了p η p_{\eta}p 
η
​
 和p θ p_{\theta}p 
θ
​
 组件以及训练和解码过程。
————————————————
版权声明：本文为CSDN博主「多喝开水少熬夜」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_52695557/article/details/134307931




论文解读https://blog.csdn.net/m0_52695557/article/details/134307931

RAG（检索增强生成）技术详解：基于垂直领域专有数据的Chatbots是如何实现的
https://dbaplus.cn/news-160-5633-1.html
