ReadAgent是模仿人类阅读过程，先分段摘要再回忆，谷歌新框架ReadAgent在三个长文档阅读理解数据集上取得了更强的性能，有效上下文提升了3-20倍。项目地址是：https://github.com/microsoft/JARVIS。 项目名称借鉴了漫威《钢铁侠》中人工智能管家的名称，也传递了项目想要实现通用人工智能的愿景。

基于Transformer的大语言模型（LLM）具有很强的语言理解能力，但LLM一次能够读取的文本量仍然受到极大限制。
除了上下文窗口较小外，LLM的性能会随着输入内容长度的增加而下降，即便输入内容未超过模型的上下文窗口长度限制也是如此。相比之下，人类却可以阅读、理解和推理很长的文本。LLM和人类在阅读长度上存在差异的主要原因在于阅读方法：LLM逐字地输入精确的内容，并且该过程相对被动；但过于准确的信息往往会被遗忘，而阅读过程更注重理解模糊的要点信息，即不考虑准确单词的内容能记忆更长时间。
人类阅读也是一个互动的过程，比如回答问题时还需要从原文中进行检索。

为了解决这些限制，来自Google DeepMind和Google Research的研究人员提出了一个全新的LLM系统ReadAgent，受人类如何交互式阅读长文档的启发，将有效上下文长度增加了20倍。
## ReadAgent框架介绍
![example](/image/ReadAgent/PaginationPrompt.png)  
### 1、要点记忆（gist memory）
要点记忆是原始长上下文中文本块的短要点的有序集合，构建gist记忆有两个步骤：分页（pagination）和记忆提要（memory gisting）。  
#### 片段分页（episode pagination）
当ReadAgent阅读长文本时，通过选择暂停阅读的位置来决定在记忆片段中存储哪些内容。
每一步都会为LLM提供部分文本，从上一个暂停点开始，并在达到最大单词数限制时结束；提示LLM选择段落之间的哪个点将是自然的暂停点，然后将前一个和当前暂停点之间的内容视为一个episode，也可以叫做页（page）。
![example](/image/ReadAgent/workflow.png)  
#### 记忆提要（memory gisting）
对于每一页，提示LLM将确切的内容缩短为要点或摘要。

![example](/image/ReadAgent/GistingPrompt.png)  
目前的LLM技术仍然不完善，在构建 AGI 系统的过程中面临着一系列挑战：
- 受限于文本生成的输入和输出形式，当前的 LLM 缺乏处理视觉和语音等复杂信息的能力。
- 在现实世界的场景中，一些复杂的任务通常由多个子任务组成，因此需要多个模型的调度和协作。
- 对于一些具有挑战性的任务，LLM 仍然比一些专家（例如，微调模型）弱。
如何解决这些问题，可能是 LLM 迈向 AGI 系统的第一步，也是关键一步。  

### 2、并行和顺序交互查找
由于要点记忆与页相关，所以只需提示LLM来找出哪一页更像是答案，并在给定特定任务的情况下再次阅读，主要有两种查找策略：同时并行查找所有页面（ReadAgent-P）和每次查找一个页面（ReadAgent-S）。
#### ReadAgent-P
比如说，在问答任务中，通常会给LLM输入一个可以查找的最大页数，但也会指示其使用尽可能少的页面，以避免不必要的计算开销和干扰信息（distracting information）。
![example](/image/ReadAgent/Parallel_lookup_Prompt.png)  

#### ReadAgent-S
顺序查找策略中，模型一次请求一页，在决定展开（expand）哪个页面之前，先查看之前展开过的页面，从而使模型能够访问比并行查找更多的信息，预期在某些特殊情况下表现得更好。
但与模型的交互次数越多，其计算成本也越高。
![example](/image/ReadAgent/Sequential_Lookup_Prompt.png)  

### 3、计算开销和可扩展性
片段分页、记忆提要和交互式查找需要迭代推理，也存在潜在的计算开销，但具体开销由一个小因子线性约束，使得该方法的计算开销不会输入长度的增加而剧烈提升。
由于查找和响应大多是条件要点（conditioned gists）而非全文，所以在同一上下文中的任务越多，成本也就越低。


### 4、ReadAgent变体
当使用长文本时，用户可能会提前知道要解决的任务：在这种情况下，提要步骤可以在提示中包括任务描述，使得LLM可以更好地压缩与任务无关的信息，从而提高效率并减少干扰信息，即条件ReadAgent
更通用的任务设置下，在准备提要时可能不知道具体任务，或者可能知道提出的要点需要用于多个不同的任务，例如回答关于文本的问题等。
因此，通过排除注册步骤中的任务，LLM可以产生更广泛有用的提要，代价是减少压缩和增加干扰注意力的信息，即非条件ReadAgent。
这篇论文中只探讨了无条件设置，但在某些情况下，条件设置可能更有优势。


### 迭代提要（iterative gisting）
对于一段很长的事件历史，例如对话等，可以考虑通过迭代提要来进一步压缩旧记忆来实现更长的上下文，对应于人类的话，旧记忆更模糊。


## 实验结果
研究人员评估了ReadAgent在三个长上下文问答挑战中的长文档阅读理解能力：QuALITY、NarrativeQA和QMSum。
虽然ReadAgent不需要训练，但研究人员仍然选择在训练集上开发了一个模型并在验证、测试和/或开发集上进行了测试，以避免过拟合系统超参数的风险。
选用的模型为指令微调后的PaLM 2-L模型。
评估指标为压缩率（compression rate, CR），计算方法如下：



### LLM评分器
NarrativeQA和QMSum都有一个或多个自由形式的参考回复，通常使用诸如ROUGE-F之类的语法匹配度量来评估。
除此之外，研究人员使用自动LLM评分器来评估这些数据集，作为人工评估的替代方法。
![example](/image/ReadAgent/Strict_Permissive_LLM_Rater_Prompt.png)  

上面两个提示中，「严格LLM评分器提示」用于判断是否存在精确匹配，「许可LLM评分器提示」用于判断是否存在精确匹配或部分匹配。
基于此，研究人员提出了两个评价指标：LLM-Rating-1（LR-1）是一个严格的评估分数，计算所有示例中精确匹配的百分比；LLM-Rating-2（LR-2）计算精确匹配和部分匹配的百分比。

### 长上下文阅读理解

#### QuALITY
QuALITY是一个多选问答任务，每个问题包含四个答案，使用来自多个不同来源的文本数据。
![example](/image/ReadAgent/Histogram_of_QuALITY.png) 

实验结果显示，ReadAgent（查找1-5页）实现了最好的结果，压缩率为66.97%（即提要后上下文窗口中可以容纳3倍的token）。
当增加允许查找的最大页数（最多5页）时，性能会不断提高；在6页时，性能开始略有下降，即6页上下文可能会增加干扰信息。

![example](/image/ReadAgent/QuALITY_results_on_the_dev_set.png) 

#### NarrativeQA
在三个阅读理解数据集中，NarrativeQA的平均上下文长度最长，为了将gists放入上下文窗口，需要扩展页面的尺寸大小。
提要对Gutenburg文本（书籍）的压缩率为96.80%，对电影剧本的压缩率为91.98%。
![example](/image/ReadAgent/NarrativeQA_results.png) 

#### QMSum
QMSum由各种主题的会议记录以及相关问题或说明组成，长度从1,000字到26,300字不等，平均长度约为10,000字，其答案是自由形式的文本，标准的评估指标是ROUGE-F
![example](/image/ReadAgent/QMSum_validation_results.png) 
可以看到性能随着压缩率的降低而提高，因此查找更多页面的技术往往比查找更少页面的技术做得更好。
还可以看到ReadAgentS大大优于ReadAgent-P（以及所有基线），性能改进的代价是检索阶段的请求数量增加了六倍。










## 实例讲解
解决不同领域和模态的AI任务是迈向通用人工智能的关键一步。虽然现在有大量的AI模型可以用于解决不同的领域和模态的问题，但是它们不能解决复杂的AI问题。由于大模型(LLM)在语言理解、生成、交互和推理上展现出很强的能力，因此构建HuggingGPT框架，连接不同的AI模型来解决AI任务。ChatGPT充当一个控制器的作用来管理现有的AI模型以解决复杂的AI任务，以语言为一个通用的接口来启动AI处理这些任务。  
下面我们以一个真实的用户问题实例，来进行讲解：  
![example](/image/huggingGPT-example.png)  
我们看一下用户问题：你能描述一下这张照片是什么？以及图片中的有多少个实体？问题和这张照片作为输入，传递到chatGPT，通过语义理解和分析，主要是和图像相关，规划了图像相关的几个任务。
然后，在HuggingFace中选择对应任务的的模型，像图像识别、实体检测等。接着，根据任务，执行相应的模型，并将结果返回给ChatGPT。最后，ChatGPT利用返回各个模型的返回的结果，生成答案，包括两部分：图片描述为“一群长颈鹿和斑马在田野里吃草”，实体检测结果是“有5个，分别是XX及得分”，还把对应的动物用红框框了出来；并且还会说明是怎么得到这个结果，分别使用了 image classification, object detection和image caption三个任务，以及对应的任务所选择的模型。  
根据任务讲解的实际步骤，整个HuggingGPT的过程可以分为四个阶段：
- 任务规划：使用ChatGPT分析用户的请求，了解用户的意图。通过提示将它们拆解为可能的可解决任务。
- 模型选择：为了解决计划任务，ChatGPT选择托管在基于模型描述的Hugging Face。
- 任务执行：调用并执行每个选择的模型，并将结果返回给ChatGPT。
- 响应生成：最后，利用ChatGPT集成来自所有模型的预测并为用户生成响应。

## HuggingGPT详细说明
HuggingGPT 可以看作是一个协作系统，以ChatGPT为控制器，以专家模型为执行者的一个合作系统，整体的工作流为：任务规划、模型选择、任务执行和回答生成。过程描述如下：
1. ChatGPT首先解析用户请求，将其分解为多个任务，并根据其知识规划任务顺序和依赖关系。
2. ChatGPT根据 Hugging Face中的模型描述，将解析后的任务分配给对应的专家模型。
3. 专家模型在推理端点上执行分配的任务，并将执行信息和推理结果返回给ChatGPT。
4. 最后，ChatGPT总结执行过程日志和推理结果，并将信息整合返回给用户。

### 如何规划任务
在第一个阶段，大型语言模型接受用户的请求，将其分解为一系列结构化任务。  
复杂的请求通常涉及多个任务，大型语言模型需要确定这些任务的依赖关系和执行顺序。为了提示大型语言模型进行有效的任务规划，HuggingGPT在其提示设计中采用了基于规范的指令和基于演示的解析。  
基于规范的指令任务规范为任务提供了统一的模板，并允许大型语言模型通过槽位归档进行任务解析。HuggingGPT 为任务解析设计了四个接口，分别是任务类型、任务ID、任务依赖项和任务参数：
![example](/image/task-planning.png)
- task id：任务 ID 为任务规划提供了一个唯一的标识符，用于引用相关任务及其生成的资源。
- task type：任务类型涵盖语言、视觉、视频、音频等不同任务。HuggingGPT当前支持的任务列表如下表所示。
- task dependencies：任务相关性定义了执行所需的先决条件任务。只有在完成所有先决条件相关任务后，才会启动该任务。
- task arguments：任务参数包含执行任务所需的参数列表。它包含三个子字段，根据任务类型填充文本、图像和音频资源。它们是从用户的请求或相关任务的生成资源中解析的。不同任务类型的相应参数类型如下表所示。   

在模板中规定了task的类型范围，HuggingGPT融合了HuggingFce中成百上千的模型和GPT，可以解决24种任务，包括文本分类、对象检测、语义分割、图像生成、问答、文本语音转换和文本视频转换。支持的任务列类型表如下：
![example](/image/task.png)

### 如何选择模型
解析任务列表后，HuggingGPT 接下来需要匹配任务和模型，即为任务列表中的每个任务选择合适的模型。  
为此，我们首先从 HuggingFace Hub 获得专家模型的描述，然后通过上下文中的任务模型分配机制为任务动态选择模型。这种做法允许增量模型访问（简单地提供专家模型的描述），并且更加开放和灵活。  
HuggingFace Hub 上托管的专家模型附有全面的模型描述，这些描述通常由开发人员提供。这些描述包含有关模型的功能、体系结构、支持的语言和域、许可等方面的信息。这些信息有效地支持 HuggingGPT 根据用户请求和模型描述的相关性为任务选择正确模型的决定。task描述如下：  
![example](/image/task-description.png)
在上下文任务模型分配中，我们将任务和模型的分配视为单选问题，其中潜在模型作为给定上下文中的选项呈现。通过在提示中包括用户查询和解析的任务，HuggingGPT 可以为手头的任务选择最合适的模型。然而，由于最大上下文长度的限制，不可能总是在提示中包含所有相关的模型信息。为了解决这个问题，我们根据模型的任务类型（task type）筛选模型，只保留与当前任务类型匹配的模型，并且对剩下的模型根据下载量排序，然后选择top-K个模型作为HuggingGPT的候选模型。  
![example](/image/model-selection.png)

### 如何执行任务
任务被分配给特定的模型后，接下来就是执行任务，即执行模型推理。  
为了加速和计算稳定性，HuggingGPT 在混合推理端点上运行这些模型。通过将任务参数作为输入，模型计算推理结果，然后将其发送回大型语言模型。为了进一步提高推理效率，可以对不具有资源依赖性的模型进行并行化。这意味着可以同时启动满足先决条件依赖关系的多个任务。
- 混合端点：理想的场景是我们只在HuggingFace上使用推理端点。为了保持系统的稳定和高效，HuggingGPT在本地提取并运行一些常见或耗时的模型。局部推理端点很快，但覆盖的模型较少，而HuggingFace的推理端点则相反。因此，本地端点比HuggingFace的推理端点具有更高的优先级。只有在匹配的模型未在本地部署的情况下，HuggingGPT才会在HuggingFace端点上运行该模型。
- 资源依赖性：尽管HuggingGPT能够通过任务规划来开发任务顺序，但在任务执行阶段有效管理任务之间的资源依赖性仍然具有挑战性。因为HuggingGPT无法在任务规划阶段为任务指定未来生成的资源。为了解决这个问题，我们使用一个唯一的符号```<resource>```来管理资源依赖关系。具体来说，HuggingGPT将先决任务生成的资源标识为```<resource-task_id>```，其中task_id是先决任务的任务id。在任务规划阶段，如果存在依赖于task_id任务生成的资源的任务，则HuggingGPT将此符号设置为任务参数中相应的资源子字段。然后在任务执行阶段，HuggingGPT将此符号动态替换为先决任务生成的资源。此策略使HuggingGPT能够在任务执行期间有效地处理资源依赖关系。

### 如何生成回答
在所有任务执行完成后，HuggingGPT 进入响应生成阶段。  
在这个阶段，HuggingGPT将前三个阶段（任务规划、模型选择和任务执行）的所有信息集成在一起，包括计划任务的列表、为任务选择的模型以及模型的推理结果。当然，最重要的是推理结果，这是 HuggingGPT做出最终决策的支持。这些推理结果以结构化格式出现，例如对象检测模型中具有检测概率的边界框、问答模型中的答案分布等。HuggingGPT允许LLM接收这些推理结果作为输入，并将具有置信度的响应汇总，返回给用户。  
![example](/image/response.png)

## 限制与不足
尽管HuggingGPT 提出了人工智能解决方案的新范式，但仍然存在一些局限性或改进空间： 
- 任务规划严重依赖LLM的能力：大模型并不能100%确保生成的计划始终是可行且最优的，因此，探索优化LLM的方法至关重要，进而提升规任务划能力。
- 效率：效率的瓶颈主要在使用大型语言模型的推理上。对于每一轮用户请求，HuggingGPT 都需要在任务规划、模型选择和响应生成阶段至少与大型语言模型进行一次交互，大大增加了响应延迟，并导致用户体验下降。 
- 最大上下文长度的限制：受 LLM 可以接受的最大令牌数量的限制，HuggingGPT 面临着最大上下文长度的限制。我们使用了会话窗口，在任务规划阶段只跟踪了会话上下文来缓解它。 
- 系统稳定性：一是在大型语言模型的推理过程中通常是不可控的。大型语言模型在推理时偶尔会不符合指令，并且输出格式可能会超出预期，从而导致程序工作流中出现异常。二是HuggingFace的推理端点上托管的专家模型的状态不可控。HuggingFace上的专家模型可能会受到网络延迟或服务状态的影响，导致任务执行阶段出现错误。

## 总结
HuggingGPT以语言为接口，将LLM与人工智能模型连接起来，来系统解决人工智能任务。    
LLM可以被视为管理人工智能模型的控制器，并且可以利用HuggingFace等机器学习社区的模型来解决用户的不同请求。通过利用LLM在理解和推理方面的优势来剖析用户的意图，并将任务分解为多个子任务。然后，基于专家模型描述，HuggingGPT为每个任务分配最合适的模型，并集成不同模型的结果。通过利用来自机器学习社区的众多人工智能模型的能力，HuggingGPT在解决具有挑战性的人工智能任务方面展现了巨大的潜力。

## 使用说明
HuggingGPT已经在GitHub上公开，并且持续在演进，地址如下：https://github.com/microsoft/JARVIS。  
![example](/image/github.png)  
从服务器下载和运行 HuggingGPT ：  
```
# setup env
cd server
conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch 
pip install git+https://github.com/huggingface/diffusers.git
pip install git+https://github.com/huggingface/transformers
pip install -r requirements.txt

# download models
cd models
sh download.sh

# run server
cd ..
python bot_server.py
python model_server.py
```

从 web 运行 HuggingGPT 的方法如下：


```
cd web
npm install
npm run dev
```
## 参考资料
论文下载地址：https://arxiv.org/pdf/2303.17580.pdf    
开源代码：https://github.com/microsoft/JARVIS

